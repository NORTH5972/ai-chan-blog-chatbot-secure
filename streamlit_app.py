import streamlit as st
import json
import re
import os
import numpy as np
from sentence_transformers import SentenceTransformer
import google.generativeai as genai
import streamlit_authenticator as stauth 

# --- 1. Google Gemini API ã‚­ãƒ¼ã®è¨­å®š ---
API_KEY = os.getenv("GOOGLE_API_KEY") 
if not API_KEY and "GOOGLE_API_KEY" in st.secrets: 
    API_KEY = st.secrets["GOOGLE_API_KEY"]

if not API_KEY:
    st.error("ã‚¨ãƒ©ãƒ¼: Google Gemini API ã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    st.error("Streamlit Cloudã®Secretsã¾ãŸã¯ç’°å¢ƒå¤‰æ•°ã« 'GOOGLE_API_KEY' ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop() 

genai.configure(api_key=API_KEY)

# === å…±é€šã®åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã¨LLMãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰é–¢æ•° (st.cache_resource ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥) ===
@st.cache_resource
def get_llm_model():
    st.info("LLMãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    model = genai.GenerativeModel('models/gemini-1.5-pro-latest') 
    st.success("LLMãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    return model

@st.cache_resource
def get_query_embedding_model(model_name="intfloat/multilingual-e5-small"):
    st.info("ã‚¯ã‚¨ãƒªåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    model = SentenceTransformer(model_name)
    st.success("ã‚¯ã‚¨ãƒªåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    return model

# === è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–¢æ•° ===
def load_and_clean_data(file_path):
    cleaned_articles = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            articles = json.load(f)
        
        for article in articles:
            body = article.get('body', '') 
            body = re.sub(r'http://googleusercontent\.com/youtube\.com/\d+', '', body)
            body = re.sub(r'https?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\.(?:jpg|jpeg|png|gif|bmp|webp)', '', body) 
            body = re.sub(r'ï»¿', '', body) 
            body = re.sub(r'\n\s*\n', '\n', body) 
            body = re.sub(r'\s{2,}', ' ', body)   
            body = body.strip()
            if len(body) < 20: 
                continue
            cleaned_articles.append({
                'title': article.get('title'),
                'date': article.get('date'),
                'url': article.get('url'),
                'body': body 
            })
        return cleaned_articles
    except FileNotFoundError:
        st.error(f"ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ« '{file_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return []
    except json.JSONDecodeError:
        st.error(f"ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ« '{file_path}' ãŒä¸æ­£ãªJSONå½¢å¼ã§ã™ã€‚")
        return []
    except Exception as e:
        st.error(f"ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¾ãŸã¯ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return []

# === åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã¨è¨˜äº‹ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ– ===
@st.cache_resource
def load_or_generate_embeddings(articles, _embedding_model_instance):
    if os.path.exists(EMBEDDINGS_FILE) and os.path.exists(METADATA_FILE):
        try:
            existing_embeddings = np.load(EMBEDDINGS_FILE)
            with open(METADATA_FILE, 'r', encoding='utf-8') as f:
                existing_metadata = json.load(f)
            
            if len(existing_metadata) == len(articles) and existing_embeddings.shape[0] == len(articles):
                st.info("æ—¢å­˜ã®åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ã¨è¨˜äº‹ãƒ‡ãƒ¼ã‚¿æ•°ãŒä¸€è‡´ã—ã¾ã™ã€‚å†ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                return existing_embeddings, existing_metadata
            else:
                st.warning("æ—¢å­˜ã®åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ã¨è¨˜äº‹ãƒ‡ãƒ¼ã‚¿æ•°ãŒä¸€è‡´ã—ãªã„ãŸã‚ã€åŸ‹ã‚è¾¼ã¿ã‚’å†ç”Ÿæˆã—ã¾ã™ã€‚")
        except Exception as e:
            st.warning(f"æ—¢å­˜ã®åŸ‹ã‚è¾¼ã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ ({e})ã€‚åŸ‹ã‚è¾¼ã¿ã‚’å†ç”Ÿæˆã—ã¾ã™ã€‚")

    st.info("åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆä¸­... (åˆå›ã¯ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒç™ºç”Ÿã—ã¾ã™)")
    
    corpus = [article['body'] for article in articles]
    embeddings = _embedding_model_instance.encode(corpus, batch_size=32, show_progress_bar=False) 
    
    st.info("ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

    metadata = []
    for i, article in enumerate(articles): 
        metadata.append({
            'original_index': i, 
            'title': article['title'], 
            'url': article['url'], 
            'date': article['date'], 
            'body_preview': article['body'][:100].replace('\n', ' ').strip() + '...' if len(article['body']) > 100 else article['body'].strip()
        })
    
    np.save(EMBEDDINGS_FILE, embeddings)
    with open(METADATA_FILE, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=4)
    
    st.success(f"åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ '{EMBEDDINGS_FILE}' ã«ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ '{METADATA_FILE}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
    
    return embeddings, metadata

# === RAGChatbotã‚¯ãƒ©ã‚¹ ===
class RAGChatbot:
    def __init__(self, embeddings, metadata, articles_data, llm_model, query_embedding_model):
        self.embeddings = embeddings 
        self.metadata = metadata     
        self.articles_data = articles_data 
        
        self.llm_model = llm_model 
        self.query_embedding_model = query_embedding_model 

    def find_relevant_articles(self, query, top_k=3):
        query_embedding = self.query_embedding_model.encode([query])[0]
        
        norms_embeddings = np.linalg.norm(self.embeddings, axis=1)
        norm_query = np.linalg.norm(query_embedding)
        
        similarities = np.zeros(len(self.embeddings))
        non_zero_indices = np.where(norms_embeddings != 0)[0]
        if norm_query != 0:
            similarities[non_zero_indices] = np.dot(self.embeddings[non_zero_indices], query_embedding) / (norms_embeddings[non_zero_indices] * norm_query)
        
        top_k_indices = np.argsort(similarities)[::-1][:top_k]
        
        relevant_articles = []
        for i_metadata in top_k_indices:
            original_article_index = self.metadata[i_metadata]['original_index']
            if 0 <= original_article_index < len(self.articles_data):
                relevant_articles.append(self.articles_data[original_article_index])
            else:
                st.warning(f"è­¦å‘Š: original_index {original_article_index} ãŒarticles_dataã®ç¯„å›²å¤–ã§ã™ã€‚")
            
        return relevant_articles

    def generate_response(self, query, relevant_articles):
        context = ""
        if relevant_articles:
            context += "ä»¥ä¸‹ã®ãƒ–ãƒ­ã‚°è¨˜äº‹ã®æƒ…å ±ã‚’å‚ç…§ã—ã¦è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚\n"
            for i, article in enumerate(relevant_articles):
                context += f"--- è¨˜äº‹ {i+1} (ã‚¿ã‚¤ãƒˆãƒ«: {article['title']}, æ—¥ä»˜: {article['date']}, URL: {article['url']}) ---\n"
                context += article['body'] + "\n\n"
            context += "--- å‚ç…§æƒ…å ±çµ‚ã‚ã‚Š ---\n\n"
            context += "å‚ç…§æƒ…å ±ã«åŸºã¥ã„ã¦è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚å‚ç…§æƒ…å ±ã«ãªã„å ´åˆã¯ã€ã€Œç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€ãã®æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚\n"
            context += "å›ç­”ã®æœ€å¾Œã«ã€å‚ç…§ã—ãŸè¨˜äº‹ã®ç•ªå·ã€ã‚¿ã‚¤ãƒˆãƒ«ã€æ—¥ä»˜ã€URLã‚’å¿…ãšè¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚\n"
            context += "ä¾‹ï¼š\n"
            context += "å›ç­”ï¼šã¯ã„ã€ã€‡ã€‡ã§ã™ã€‚è¨˜äº‹1ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ï¼šXXXã€æ—¥ä»˜ï¼šYYYY.MM.DDã€URLï¼šZZZï¼‰ã‚’ã”è¦§ãã ã•ã„ã€‚\n"
            context += "åŠ ãˆã¦ã€ã€‡ã€‡ã®ç‚¹ã‚‚è€ƒæ…®ã—ã¦ãã ã•ã„ã€‚\n" 
        else:
            context += "å‚ç…§ã§ãã‚‹ãƒ–ãƒ­ã‚°è¨˜äº‹æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã€‚è³ªå•ã«ç­”ãˆã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã€‚\n"
            return "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€ãã®æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚åˆ¥ã®è³ªå•ã‚’ãŠè©¦ã—ãã ã•ã„ã€‚"

        prompt = f"{context}è³ªå•: {query}\nå›ç­”:"
        
        try:
            response = self.llm_model.generate_content(prompt)
            if response.candidates and response.candidates[0].content.parts:
                return response.candidates[0].content.parts[0].text.strip()
            else:
                return "LLMã‹ã‚‰ã®å›ç­”ç”Ÿæˆã«å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
        except Exception as e:
            st.error(f"LLMã‹ã‚‰ã®å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            if "blocked_reason" in str(e): 
                return "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€ã“ã®è³ªå•ã¯ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒªã‚·ãƒ¼ã«é•åã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
            elif "Quota exceeded" in str(e): 
                return "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€APIã®åˆ©ç”¨åˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãæ™‚é–“ã‚’ãŠã„ã¦ã‹ã‚‰ãŠè©¦ã—ãã ã•ã„ã€‚"
            return "ç¾åœ¨ã€å›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã€‚æ™‚é–“ã‚’ãŠã„ã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚ (APIã‚¨ãƒ©ãƒ¼)"

# --- 6. Streamlit ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®UIã¨ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆèªè¨¼éƒ¨åˆ†ã‚’å…ˆé ­ã«ï¼‰ ---

# ãƒ•ã‚¡ã‚¤ãƒ«åã¨å®šæ•°ã‚’å®šç¾©
DATA_FILE = "endo_sakura_blog_data.json"
EMBEDDINGS_FILE = "blog_embeddings.npy"
METADATA_FILE = "blog_metadata.json"

st.set_page_config(page_title="æ„›ã¡ã‚ƒã‚“ãƒ–ãƒ­ã‚°ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ", page_icon="ğŸŒ¸")

# --- èªè¨¼æƒ…å ±ã®å®šç¾© ---
credentials = {
    "usernames": {}
}
for username in st.secrets.get("auth", {}).get("credentials", {}).get("usernames", {}):
    user_info = st.secrets["auth"]["credentials"]["usernames"][username]
    credentials["usernames"][username] = {
        "email": user_info.get("email", ""),
        "name": user_info.get("name", username),
        "password": user_info["password"]
    }

# Streamlit Authenticatorã®åˆæœŸåŒ–
authenticator = stauth.Authenticate(
    credentials,
    st.secrets.get("cookie", {}).get("name", "ai_chan_chatbot_cookie"), 
    st.secrets.get("cookie", {}).get("key", "some_default_secret_key_for_cookie"), 
    st.secrets.get("cookie", {}).get("expiry_days", 30),
)

# --- èªè¨¼UIã®è¡¨ç¤º ---
# login() ã§ã¯ãªã authenticate() ã‚’ä½¿ç”¨ã—ã€ç›´æ¥å¼•æ•°ã‚’æ¸¡ã™
name, authentication_status, username = authenticator.authenticate(
    "Login Form", # ãƒ•ã‚©ãƒ¼ãƒ å
    "main"        # è¡¨ç¤ºå ´æ‰€ ('main', 'sidebar', 'unrendered' ã®ã„ãšã‚Œã‹)
)

# èªè¨¼æˆåŠŸã®å ´åˆã®ã¿ã€ã‚¢ãƒ—ãƒªã®æ®‹ã‚Šã‚’è¡¨ç¤º
if authentication_status: 
    authenticator.logout('Logout', 'sidebar') 
    st.sidebar.write(f"ã‚ˆã†ã“ãã€{name}ã•ã‚“ï¼")

    st.title("ğŸŒ¸ æ„›ã¡ã‚ƒã‚“ãƒ–ãƒ­ã‚°ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ")
    st.write("é è—¤ã•ãã‚‰ã•ã‚“ã®ãƒ–ãƒ­ã‚°è¨˜äº‹ã‹ã‚‰æƒ…å ±ã‚’å–å¾—ã—ã¦ã€è³ªå•ã«ç­”ãˆã¾ã™ã€‚")

    @st.cache_resource
    def initialize_chatbot_instance(): 
        with st.spinner("è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­..."):
            articles_data = load_and_clean_data(DATA_FILE)
            if not articles_data:
                st.error("è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚`endo_sakura_blog_data.json` ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                return None
            st.success(f"ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã•ã‚ŒãŸè¨˜äº‹ãƒ‡ãƒ¼ã‚¿: {len(articles_data)}ä»¶")

        llm_model_instance = get_llm_model()
        query_embedding_model_instance = get_query_embedding_model()

        with st.spinner("åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆã¾ãŸã¯ãƒ­ãƒ¼ãƒ‰ä¸­..."):
            embeddings, metadata = load_or_generate_embeddings(articles_data, query_embedding_model_instance) 
            if embeddings is None or len(embeddings) == 0:
                st.error("åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ãŒç”Ÿæˆã¾ãŸã¯ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                return None
            st.success(f"åŸ‹ã‚è¾¼ã¿ã®å½¢çŠ¶: {embeddings.shape}, ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä»¶æ•°: {len(metadata)}")

        with st.spinner("RAGãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­..."):
            chatbot_instance = RAGChatbot(embeddings, metadata, articles_data, llm_model_instance, query_embedding_model_instance) 
            st.success("ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®æº–å‚™ãŒæ•´ã„ã¾ã—ãŸï¼")
        return chatbot_instance

    chatbot = initialize_chatbot_instance() 

    if chatbot is None:
        st.stop()

    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("å›ç­”ã‚’ç”Ÿæˆä¸­..."):
                relevant_articles = chatbot.find_relevant_articles(prompt, top_k=3)
                response = chatbot.generate_response(prompt, relevant_articles)
                st.markdown(response)
            st.session_state.messages.append({"role": "assistant", "content": response})

    st.sidebar.markdown("---")
    st.sidebar.markdown("é–‹ç™ºè€…æƒ…å ±")
    st.sidebar.markdown("ã“ã®ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã¯ã€Python, Streamlit, Sentence-Transformers, Google Gemini API ã‚’ä½¿ç”¨ã—ã¦æ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã™ã€‚")

# èªè¨¼å¤±æ•—ã¾ãŸã¯æœªèªè¨¼ã®å ´åˆã€ã‚¢ãƒ—ãƒªã®å®Ÿè¡Œã‚’ã“ã“ã§åœæ­¢
else: # authentication_status ãŒ False ã¾ãŸã¯ None ã®å ´åˆ
    st.session_state["authentication_status"] = authentication_status # çŠ¶æ…‹ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜
    st.session_state["name"] = name
    st.session_state["username"] = username
    
    # ã“ã“ã§ã¯ã€ãƒ­ã‚°ã‚¤ãƒ³ãƒ•ã‚©ãƒ¼ãƒ ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã®ã§ã€ç‰¹ã«ä½•ã‚‚ã—ãªã„ã€‚
    # èªè¨¼å¤±æ•—ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ authenticator.authenticate() ã®ä¸­ã§å‡¦ç†æ¸ˆã¿ã€‚
    pass